{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Library and Set Up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "## Plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "# Others\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.manifold import TSNE\n",
    "from IPython.display import clear_output\n",
    "from keras.models import model_from_yaml\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    # Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    ## Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    # Stemming\n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Train Data (Captured From Reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reddit=pd.read_csv(\"subreddit_posts.csv\", sep=',',names=['text','label'],header=None)\n",
    "train_reddit['text'] = train_reddit.fillna({'text':''})\n",
    "train_reddit\n",
    "\n",
    "train_reddit['text'] = train_reddit['text'].map(lambda x: clean_text(x))\n",
    "train_reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Train data of ERisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate=pd.read_csv(\"FinalTrain.csv\", sep=',',names=['id','text','label'],header=None)\n",
    "evaluate.fillna({'text':''})\n",
    "evaluate['text'] = evaluate['text'].map(lambda x: clean_text(x))\n",
    "evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Test Data(ERisk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv(\"FinalTest.csv\", sep=',',names=['id','text','label'],header=None)\n",
    "test\n",
    "test.fillna({'text':''})\n",
    "test['text'] = test['text'].map(lambda x: clean_text(x))\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [train_reddit['text'],evaluate['text']]\n",
    "\n",
    "totalData = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_line=[]\n",
    "lines=totalData.values.tolist()\n",
    "i=0\n",
    "\n",
    "for line in lines:\n",
    "    \n",
    "        tokenized_line.append(word_tokenize(line))\n",
    "    \n",
    "print(tokenized_line[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=300 #can be 100,300 \n",
    "w2vmodel=gensim.models.Word2Vec(sentences=tokenized_line,size=EMBEDDING_DIM,window=2,workers=15,min_count=1)\n",
    "w2vmodel.train(tokenized_line,total_examples=len(tokenized_line),epochs=10)  \n",
    "words=list(w2vmodel.wv.vocab)\n",
    "print('Vocabulary Size: ',len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer()\n",
    "frames = [train_reddit['text'],evaluate['text']]\n",
    "\n",
    "totalData = pd.concat(frames)\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(totalData)\n",
    "\n",
    "#pad sequences\n",
    "maxLength=max([len(s.split()) for s in totalData])\n",
    "print('Max-Length: ',maxLength)\n",
    "#To Reduce time we Set max length to 300\n",
    "maxLength=300\n",
    "#vocabulary Size\n",
    "vocabularySize=len(tokenizer.word_index)+1\n",
    "\n",
    "train_Token=tokenizer.texts_to_sequences(train_reddit['text'])\n",
    "evaluate_Token=tokenizer.texts_to_sequences(evaluate['text'])\n",
    "\n",
    "#padding\n",
    "train_padded=pad_sequences(train_Token,maxlen=maxLength,padding=\"post\")\n",
    "evaluate_padded=pad_sequences(evaluate_Token,maxlen=maxLength,padding=\"post\")\n",
    "\n",
    "print(train_padded.shape)\n",
    "print('---')\n",
    "print(evaluate_padded.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=vocabularySize)\n",
    "tokenizer.fit_on_texts(tokenized_line)\n",
    "seq=tokenizer.texts_to_sequences(tokenized_line)\n",
    "\n",
    "\n",
    "word_index=tokenizer.word_index\n",
    "print('Total Unique Token: ',len(word_index))\n",
    "\n",
    "\n",
    "pad=pad_sequences(seq,maxlen=maxLength)\n",
    "\n",
    "frames = [train_reddit['label'],evaluate['label']]\n",
    "\n",
    "totalLabel = pd.concat(frames).values\n",
    "print('Shape of Texts',pad.shape)\n",
    "print('Shape of Label',totalLabel.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix=np.zeros((len(word_index)+1,EMBEDDING_DIM))\n",
    "\n",
    "\n",
    "for word,i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector=w2vmodel[word]\n",
    "        embedding_matrix[i]=embedding_vector\n",
    "        \n",
    "    except KeyError:\n",
    "        pass\n",
    "print('Shape of Embedding Matrix: ',embedding_matrix.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=w2vmodel\n",
    "model['depress']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Lexicon for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = [\"pleasure\"]\n",
    "happy=model.wv.most_similar (positive=w1,topn=1000)\n",
    "happy=[word for word,score in happy]\n",
    "print(happy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = [\"depression\"]\n",
    "depwords=model.wv.most_similar (positive=w1,topn=1500)\n",
    "depwords=[word for word,score in depwords]\n",
    "print(depwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = [\"therapist\"]\n",
    "therapist_words=model.wv.most_similar (positive=w1,topn=1000)\n",
    "therapist_words=[word for word,score in therapist_words]\n",
    "print(therapist_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = [\"treatment\"]\n",
    "diagnosis_words=model.wv.most_similar (positive=w1,topn=1000)\n",
    "diagnosis_words=[word for word,score in diagnosis_words]\n",
    "print(diagnosis_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medicine=['Celexa', 'Lexapro', 'Prozac', 'Sarafem', 'Selfemra', 'ProzacWeekly', 'Luvox', 'Paxil', 'Paxil CR', 'Pexeva', 'Zoloft', 'Trintellix', 'Viibryd']\n",
    "medicine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_word=['i','I']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def textToVector(text):\n",
    "            i_word_count=0\n",
    "            medicine_count=0\n",
    "            diagnosis_words_count=0\n",
    "            therapist_words_count=0\n",
    "            depwords_count=0\n",
    "            happy_count=0\n",
    "            vc=0\n",
    "            prpc=0\n",
    "            ppc=0\n",
    "            vector=[]\n",
    "            \n",
    "            \n",
    "            \n",
    "            tokens=word_tokenize(text)\n",
    "            for word in tokens:\n",
    "                if word in i_word:\n",
    "                    i_word_count=i_word_count+1\n",
    "                tagged = nltk.pos_tag([word])\n",
    "                if tagged[0][1]=='PRP':   # PRP\tpersonal pronoun\tI, he, she\n",
    "                    prpc=prpc+1\n",
    "                if tagged[0][1]=='PRP$':# PRP$\tpossessive pronoun\tmy, his, hers\n",
    "                    ppc=ppc+1                                                                \n",
    "                        \n",
    "                                                                                    # VB\tverb, base form\ttake\n",
    "                if tagged[0][1]=='VB' or tagged[0][1]=='VBD':                        # VBD\tverb, past tense\ttook\n",
    "                    vc=vc+1\n",
    "                        \n",
    "            tokens=word_tokenize(text)\n",
    "            for word in tokens:\n",
    "                if word in medicine:\n",
    "                    medicine_count=medicine_count+1\n",
    "                if word in depwords:\n",
    "                    depwords_count=depwords_count+1\n",
    "                if word in therapist_words:\n",
    "                    therapist_words_count=therapist_words_count+1\n",
    "                if word in diagnosis_words:\n",
    "                    diagnosis_words_count=diagnosis_words_count+1\n",
    "                if word in happy:\n",
    "                    happy_count=happy_count+1\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            a=[i_word_count,prpc,ppc,vc,lee]=scaler.fit_transform(np.array([i_word_count,prpc,ppc,vc,len(tokens)]).reshape(-1, 1))\n",
    "#             print(a[0][0])\n",
    "\n",
    "\n",
    "    \n",
    "            vector.append([i_word_count[0],prpc[0],ppc[0],vc[0],1 if medicine_count>=1 else -1,1 if depwords_count>=1 else -1,1 if therapist_words_count>=1 else -1,1 if diagnosis_words_count>=1 else -1,1 if happy_count>=1 else -1,lee[0]])\n",
    "            return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from IPython.display import clear_output\n",
    "# nltk.download('punkt')\n",
    "import nltk\n",
    "def featureVector(df):\n",
    "\n",
    "    gb = df.groupby('id')\n",
    "\n",
    "    ii=0\n",
    "    eachUser_vector=[]\n",
    "    for key, item in gb:\n",
    "        #Each User\n",
    "         \n",
    "        f=gb.get_group(key)\n",
    "        data= f['text'];\n",
    "        class_label=f['label'].values\n",
    "        lines=data.values.tolist()\n",
    "        ii=ii+1\n",
    "        clear_output(wait=True)\n",
    "        print(ii)\n",
    "        each_sentenseVector=[]\n",
    "        for line in lines:\n",
    "            i_word_count=0\n",
    "            medicine_count=0\n",
    "            diagnosis_words_count=0\n",
    "            therapist_words_count=0\n",
    "            depwords_count=0\n",
    "            vc=0\n",
    "            prpc=0\n",
    "            ppc=0\n",
    "            happy_count=0\n",
    "            \n",
    "            tokens=word_tokenize(line)\n",
    "#             print(tokens)\n",
    "            try:\n",
    "                for word in tokens:\n",
    "                    if word in i_word:\n",
    "                        i_word_count=i_word_count+1\n",
    "                    tagged = nltk.pos_tag([word])\n",
    "                    if tagged[0][1]=='PRP':   # PRP\tpersonal pronoun\tI, he, she\n",
    "                        prpc=prpc+1\n",
    "                    if tagged[0][1]=='PRP$':# PRP$\tpossessive pronoun\tmy, his, hers\n",
    "                        ppc=ppc+1                                                                \n",
    "\n",
    "                                                                                        # VB\tverb, base form\ttake\n",
    "                    if tagged[0][1]=='VB' or tagged[0][1]=='VBD':                        # VBD\tverb, past tense\ttook\n",
    "                        vc=vc+1\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "                    \n",
    "                    \n",
    "            for word in tokens:\n",
    "                if word in medicine:\n",
    "                    medicine_count=medicine_count+1\n",
    "                if word in depwords:\n",
    "                    depwords_count=depwords_count+1\n",
    "                if word in therapist_words:\n",
    "                    therapist_words_count=therapist_words_count+1\n",
    "                if word in diagnosis_words:\n",
    "                    diagnosis_words_count=diagnosis_words_count+1\n",
    "                if word in happy:\n",
    "                    happy_count=happy_count+1\n",
    "                \n",
    "            each_sentenseVector.append([i_word_count,prpc,ppc,vc,medicine_count,depwords_count,therapist_words_count,diagnosis_words_count,happy_count,len(tokens)])\n",
    "#             print(each_sentenseVector)\n",
    "        eachUser_vector.append(each_sentenseVector)\n",
    "#         print('-------------',eachUser_vector)\n",
    "        \n",
    "    return eachUser_vector\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate.head(1))\n",
    "d=featureVector(evaluate)\n",
    "\n",
    "print(d)\n",
    "ff=['i', 'believe', 'we', 'get', 'it', 'next', 'week']\n",
    "for word in ff:\n",
    "    tagged = nltk.pos_tag([word])\n",
    "    print(tagged)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    ">>> nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=[]\n",
    "for user in d:\n",
    "    for totalpost in x:\n",
    "        v = pd.DataFrame(user,columns=['I','PersonalPronoun' ,'PossessivePronoun','Verb','Medicine','DepressionWord','Therapist','Diagonesis','Happiness','Length'])\n",
    "        df.append(v)    \n",
    "                   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df[0])\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for data in df:\n",
    "    I=data.I.mean()\n",
    "    PRP=data.PersonalPronoun.mean()\n",
    "    PP=data.PossessivePronoun.mean()\n",
    "    V=data.Verb.mean()\n",
    "    L=data.Length.mean()\n",
    "    \n",
    "    DEP=data.DepressionWord.sum()\n",
    "    THERAP=data.Therapist.sum()\n",
    "    DIAG=data.Diagonesis.sum()\n",
    "    MED=data.Medicine.sum()\n",
    "    HAP=data.Happiness.sum()\n",
    "    \n",
    "    l.append([I,PRP,PP,V,MED,DEP,THERAP,DIAG,MED,L])\n",
    "    \n",
    "    \n",
    "    \n",
    "t = pd.DataFrame(l,columns=['I','PersonalPronoun' ,'PossessivePronoun','Verb','Medicine','DepressionWord','Therapist','Diagonesis','Happiness','Length'])\n",
    "    \n",
    "print(t)    \n",
    "    \n",
    "temp=t   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['Medicine'] = t['Medicine'].map(lambda x: -1 if x==0 else 1)\n",
    "t['DepressionWord'] = t['DepressionWord'].map(lambda x: -1 if x==0 else 1)\n",
    "t['Therapist'] = t['Therapist'].map(lambda x: -1 if x==0 else 1)\n",
    "t['Diagonesis'] = t['Diagonesis'].map(lambda x: -1 if x==0 else 1)\n",
    "t['Happiness'] = t['Happiness'].map(lambda x: -1 if x==0 else 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# >>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "t[['I','PersonalPronoun','PossessivePronoun','Verb','Length']] = scaler.fit_transform(t[['I','PersonalPronoun','PossessivePronoun','Verb','Length']])\n",
    "\n",
    "\n",
    "\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel(df):\n",
    "\n",
    "    gb = df.groupby('id')\n",
    "\n",
    "    lab=[]\n",
    "    for key, item in gb:\n",
    "        #Each User\n",
    "        \n",
    "        f=gb.get_group(key)\n",
    "        data= f['text'];\n",
    "        class_label=f['label'].values\n",
    "        lab.append(class_label[0])\n",
    "    return lab\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label=getLabel(evaluate)\n",
    "# len(label)\n",
    "# print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metafeatures=[]\n",
    "for row in train_reddit['text']:\n",
    "    metafeatures.append(textToVector(row)[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metafeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf=np.array(metafeatures)\n",
    "mf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "nlp_input = Input(shape=(maxLength,), name='nlp_input')\n",
    "meta_input = Input(shape=(10,), name='meta_input')\n",
    "emb = Embedding(input_dim=len(word_index)+1,output_dim=EMBEDDING_DIM,weights=[embedding_matrix],input_length=maxLength,mask_zero=True,trainable=False)(nlp_input)\n",
    "nlp_out = Bidirectional(LSTM(EMBEDDING_DIM, dropout=0.3, recurrent_dropout=0.3))(emb)\n",
    "nlp_out = Dense(EMBEDDING_DIM, activation='relu')(nlp_out)\n",
    "meta_out =Dense(10, activation='relu')(meta_input)\n",
    "x = concatenate([nlp_out, meta_out])\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=[nlp_input , meta_input], outputs=[x])\n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit([train_padded, mf], train_reddit['label'].values, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,title=None,cmap=plt.cm.Purples):\n",
    "   \n",
    "\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    \n",
    "#     print('Confusion Matrix')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return ax\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=userPrediction(15,test)\n",
    "actual=d['Actual']\n",
    "predicted=d['Prediction']\n",
    "rd.append(d.values)\n",
    "cm.append([i,confusion_matrix(actual, predicted)])\n",
    "\n",
    "\n",
    "d=userPrediction(23,test)\n",
    "actual=d['Actual']\n",
    "predicted=d['Prediction']\n",
    "rd.append(d.values)\n",
    "cm.append([i,confusion_matrix(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import statistics as st\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "rw=4\n",
    "for data in rd:\n",
    "    if rw==13:\n",
    "            print('Risk Window Size: 15')\n",
    "    elif rw==14:\n",
    "            print('Risk Window Size: 23')\n",
    "    else:\n",
    "        print('Risk Window Size: ',rw)\n",
    "    rw=rw+1\n",
    "    \n",
    "    v = pd.DataFrame(data,columns=['Subject','Prediction' ,'Actual','Required Post to Decide'])\n",
    "    \n",
    "#     print(v)\n",
    "    rp=v['Required Post to Decide'].values\n",
    "\n",
    "    vv=v.values\n",
    "    pLatency=[]\n",
    "    for subject,p,ac,reqP in vv:\n",
    "        pLatency.append([p,ac,reqP,-1+(2/(1+np.power(np.e,-0.0078*(reqP-1)))) if p==1 else 0, 0.1296 if p==1 and ac==0 else 1 if p==0 and ac==1 else 0 if p==0 and ac==0 else 1*(1-(1/(1+ np.power(np.e,reqP-5)))), 0.1296 if p==1 and ac==0 else 1 if p==0 and ac==1 else 0 if p==0 and ac==0 else 1*(1-(1/(1+ np.power(np.e,reqP-50))))])\n",
    "        \n",
    "    pL = pd.DataFrame(pLatency,columns=['Prediction' ,'Actual','RequiredPost','pLatency','ERDE5','ERDE50'])\n",
    "#     print(pL)\n",
    "    actual=v['Actual']\n",
    "    actual=np.array(actual.values,dtype='int')\n",
    "    predicted=v['Prediction']\n",
    "    predicted=np.array(predicted.values,dtype='int')\n",
    "#     print(actual)\n",
    "    plot_confusion_matrix(actual, predicted, classes=['0','1'],title='Confusion Matrix')\n",
    "\n",
    "    print ('Classification Report : ')\n",
    "    print (classification_report(actual, predicted) )\n",
    "    f1=f1_score(actual, predicted, average='weighted') \n",
    "    print ('Accuracy Score :',accuracy_score(actual, predicted) )\n",
    "\n",
    "    print('F1 Score: ',f1)\n",
    "    \n",
    "    TPositivereqP=[]\n",
    "    TPositiveUser=[]\n",
    "    for subject,p,ac,reqP in vv:\n",
    "        if p == 1:\n",
    "            TPositivereqP.append(reqP)\n",
    "#             TPositiveUser +=1  \n",
    "    for pLat,pred in zip(pL['pLatency'],pL['Prediction']):\n",
    "        if pred==1:\n",
    "            TPositiveUser.append(pLat)\n",
    "            \n",
    "    pLatMedian=st.median(TPositiveUser)\n",
    "            \n",
    "\n",
    "    \n",
    "    print('Average Latency: ', st.median(TPositivereqP))\n",
    "            \n",
    "    print('F1 Latency: ',f1*(1-pLatMedian))\n",
    "    print('ERDE5: ',sum(pL['ERDE5'].values)/len(rp))\n",
    "    print('ERDE50: ',sum(pL['ERDE50'].values)/len(rp))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('\\n\\n<----------------------->\\n\\n')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
